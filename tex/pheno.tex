\section{Simulation of hadron collisions}

In order to analyze complicated processes at huge machines like the LHC, software that virtually reproduces the physics experiment is necessary. In order to account for stochastic effects, Monte Carlo (MC) methods are used to simulate a large number random events.  In the actual experiment, ATLAS detects collisions produced by the LHC and stores these \emph{events} in a data acquisition system. In the virtual simulation, event generators such as \hw~\cite{Herwig} and \py\cite{pythia6} produce final state particles. These particles are then run through a detector simulation of ATLAS built with \textsc{Geant 4}. The simulated and actual detector signals can then share the same event reconstruction framework and analysis. This allows a clear understanding of how the input physics is distorted step-by-step as it goes through the detector and reconstruction. 

The distortions resulting from detector imperfections and reconstruction are particularly important to this thesis, so the following chapters will employ specific terms to refer to different aspects of the simulation process. \emph{Generator-level} or \emph{truth} particles refer to those produced by the MC generator before any detector interactions. \emph{Detector-level} or \emph{reconstructed} particles refer to those that have gone through the detector simulation and been reconstructed. 

To go backwards from the (distorted) reconstructed to truth particles, the distortions introduced by the detector must be reversed. This correction process is referred to as \emph{unfolding} and is further discussed in Chapter~\ref{ch:unfolding}. Unfolding is necessary to compare actual data measured with the detector to theoretical predictions produced by generators.
\section{Monte Carlo generators}
In MC event generators, events are produced step-by-step, with random numbers pulled from quantum mechanical probability distributions at various stages\cite{PDG,Sjostrand:2009ad}.  Averaging over a large number of events gives the expected final distribution of events. The goal is to start with a QFT matrix element in a form similar to Equation~\ref{eq:ab} and produce \emph{final state}, stable particles that can be measured with a detector. The method outlines below uses a set of rules at each step to allow the iterative construction of an increasingly complex final state, resulting in hundreds of particles traveling in different directions. Since each particle has $\mathcal{O}(10)$ degrees of freedom (mass, momentum, lifetime, flavor, etc.), each event involves thousands of choices. The result of the simulation must accurately describe the average final state particles as well as fluctuations around the average. 


The steps to generate an event at the LHC can be summarized as follows:
\begin{itemize}
\item Two initial incoming protons are considered as a bag of partons with momentum distributed according to the proton PDFs and specified center of mass energy (in the case of this thesis, 8 \tev).
\item Two partons, one from each proton, are collided to give the hard process of interest: $ug \rarrow ug, u\bar{d}\rarrow W^{+}$, etc. If unstable particles such as top quark or $W/Z$ boson are produced, their decay is treated as part of the hard process in order to properly transfer properties such as spin correlations.
\item Just as electromagnetic charges can emit bremsstrahlung, color charges (i.e. partons) can emit QCD radiation. These emissions are collectively called the \emph{parton shower} (PS). Radiation emitted by partons before the collision is called Initial State Radiation (ISR). Radiation emitted by partons after the collision is called Final State Radiation (FSR).  
\item Since the proton is made up of many partons, further parton pairs may collide within a single proton-proton collision, called multiparton interactions (MPI). Each of these further collisions may have ISR or FSR. MPI is different from pileup events, when several protons collide within a single bunch crossing. 
\item After the parton collision, most of the energy remains in the beam remnants, which continue to travel in the original direction and carry color. As the partons from the collision recede, confinement forces come to dominate. These fields cannot be sufficiently described from first principles, so a model must be introduced to describe the evolution of partons into primary hadrons, a process known as \emph{hadronization}. For example, \py\ uses a \emph{string} cluster method, where the confinement field is modeled as a string stretched between each color and its anticolor. Strings are then stretched until it breaks and creates a new pair. This process is repeated until the string energy is sufficiently low-energy and the quarks from adjacent breaks produce primary hadrons. The \emph{cluster} model used by \hw\ groups quark pairs into colorless clusters. These clusters then decay into other colorless clusters or SM hadrons until only SM hadrons remain.
\item Many of these primary hadrons are unstable and decay at different timescales. Some of these decays take place within the detector volume. For ATLAS, particles with $c\tau > 10$ mm are considered \emph{final state} particles. Particles below this threshold are decayed by the event generator. 
\item At this point, the final state particles can be passed on to the detector simulation framework. Experimental information can be used to reconstuct what happened at the core of the process.
\end{itemize}

Produced via the above steps, the cross section for the range of final states produced a hard process of physics interest can be represented schematically as:
\begin{equation}
\sigma_{\text{final state}}=\sigma_{\text{hard process}} \mathcal{P}_{\text{tot, hard process \rarrow final state}}
\label{eq:finalst}
\end{equation}
This equation must be properly integrated over phase-space as well as summed over all possible decay paths that lead to a given final state. The hard process $\sigma_{\text{hard process}}$ can be calculated as in Equation~\ref{eq:ab}. The steps to reach the final state from the hard process are treated probabilistically.

nd \pow \cite{Powheg,Powheg2,Powheg3,Powheg4} can only generate a fixed order calculation of the hard process and must be interfaced to another generator such as for \py\ or \hw for the PS. 

Merging the hard process calculation with a PS program can be tricky. Both may produce wide angle radiation, so care must be taken not to double count partons. For NLO+PS, double counting can happen because PS programs attempt to emulate effects of a true NLO calculate. Two different methods are used in \textsc{MC@NLO}\ and \pow\ to avoid double-dounting. \textsc{MC@NLO} uses an analytic computation to identify what parts of the NLO calculation is already present in the PS and then subtracting this portion from the shower before combining. This method requires a new calculation for each PS program since each PS uses different NLO approximations. For \pow, the probability of each iterative spread of the shower is modified for the first emission such that exact NLO accurancy is reached. Then, any PS program can be used to shower the rest of the event with a \pt\ veto to ensure that the PS program doesn't produce any emissions harder than those from the NLO.

Finally, ``afterburner'' programs can be used to more accurately redecay special particles, such as \textsc{Tauola} for taus\cite{Jadach:1993hs} or \textsc{EvtGen} for heavy flavor hadrons\cite{Lange:2001uf}. Though not relevant for this thesis, studies of the effect of \textsc{EvtGen} on the modelling of heavy flavor decays and hadronization is presented in Appendix~\ref{app:evtgen}.
\section{Jets}
why antikt is good